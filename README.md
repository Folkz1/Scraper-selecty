# Selecty Scraper Automation

Scraper automatizado para o portal de vagas Selecty com API REST para integraÃ§Ã£o com n8n e outras ferramentas de automaÃ§Ã£o.

## ğŸš€ Funcionalidades

- **Scraper Automatizado**: Extrai vagas do portal Selecty com autenticaÃ§Ã£o automÃ¡tica
- **API REST**: Endpoints para execuÃ§Ã£o via HTTP com autenticaÃ§Ã£o por token
- **Rate Limiting**: ProteÃ§Ã£o contra uso excessivo (10 requests/hora por IP)
- **Docker Ready**: ContainerizaÃ§Ã£o completa para deploy fÃ¡cil
- **Status da Vaga**: Extrai o status de cada vaga (ex: "Vaga em processo seletivo")
- **FormataÃ§Ã£o Completa**: Dados estruturados e texto formatado para planilhas
- **Cache de Resultados**: Armazena Ãºltimo resultado para consulta rÃ¡pida
- **Health Check**: Monitoramento de saÃºde da aplicaÃ§Ã£o

## ğŸ“Š Dados ExtraÃ­dos

Para cada vaga, o scraper extrai:

- Cargo
- Empresa
- Status da vaga
- SalÃ¡rio
- Jornada de trabalho
- Tipo de contrato
- BenefÃ­cios
- DescriÃ§Ã£o das atividades
- ExperiÃªncias e qualificaÃ§Ãµes necessÃ¡rias
- Escolaridade
- NÃ­vel de atuaÃ§Ã£o
- Ãrea de atuaÃ§Ã£o
- LocalizaÃ§Ã£o
- ObservaÃ§Ãµes

## ğŸ›  InstalaÃ§Ã£o Local

### PrÃ©-requisitos

- Node.js 18+
- Docker (opcional)

### ConfiguraÃ§Ã£o

1. Clone o repositÃ³rio:
```bash
git clone https://github.com/Folkz1/Scraper-selecty.git
cd Scraper-selecty
```

2. Instale as dependÃªncias:
```bash
npm install
```

3. Configure as variÃ¡veis de ambiente:
```bash
cp .env.example .env
# Edite o arquivo .env com suas credenciais
```

4. Execute localmente:
```bash
npm start
```

A API estarÃ¡ disponÃ­vel em `http://localhost:3000`

## ğŸ³ Deploy com Docker

### Build e execuÃ§Ã£o local

```bash
# Build da imagem
docker build -t selecty-scraper .

# Executar container
docker run -d \
  --name selecty-scraper \
  -p 3000:3000 \
  --env-file .env \
  selecty-scraper
```

### Docker Compose

```bash
docker-compose up -d
```

## â˜ï¸ Deploy no EasyPanel

1. **Conectar repositÃ³rio GitHub**:
   - Acesse seu EasyPanel
   - Crie nova aplicaÃ§Ã£o
   - Conecte ao repositÃ³rio GitHub

2. **Configurar variÃ¡veis de ambiente**:
   ```
   SELECTY_EMAIL=seu_email@exemplo.com
   SELECTY_PASSWORD=sua_senha
   SELECTY_LOGIN_URL=https://selecty.app/login
   SELECTY_VACANCY_URL=https://selecty.app/vacancy/lists/index
   SCRAPER_TIMEOUT=30000
   SCRAPER_HEADLESS=true
   PORT=3000
   API_KEY=sua_chave_api_segura
   NODE_ENV=production
   ```

3. **Deploy automÃ¡tico**:
   - EasyPanel detectarÃ¡ o Dockerfile
   - Build e deploy serÃ£o automÃ¡ticos
   - A aplicaÃ§Ã£o ficarÃ¡ disponÃ­vel na URL fornecida

## ğŸ“¡ API Endpoints

### AutenticaÃ§Ã£o

Todas as rotas requerem autenticaÃ§Ã£o via Bearer token:

```bash
Authorization: Bearer YOUR_API_KEY
```

### Endpoints DisponÃ­veis

#### `POST /api/scrape`
Executa o scraper e retorna todas as vagas extraÃ­das.

**Resposta de sucesso:**
```json
{
  "success": true,
  "timestamp": "2025-11-13T20:28:08.934Z",
  "executionTime": "45s",
  "totalVacancies": 39,
  "extractedVacancies": 25,
  "successRate": "64%",
  "statusDistribution": {
    "Vaga em processo seletivo": 24,
    "Status nÃ£o informado": 1
  },
  "vacancies": [
    {
      "cargo": "Marceneiro",
      "empresa": "H7 DESIGN DE INTERIORES",
      "statusVaga": "Vaga em processo seletivo",
      "salario": "R$ 3.800,00",
      "formattedText": "â”â”â”â”â”â”â”â”â”RESUMO DA VAGAâ”â”â”â”â”â”â”â”â”\n..."
    }
  ]
}
```

#### `GET /api/scrape/status`
Retorna o status atual do scraper.

```json
{
  "success": true,
  "isRunning": false,
  "lastExecution": "2025-11-13T20:28:08.934Z",
  "hasCache": true,
  "lastSuccess": true
}
```

#### `GET /api/scrape/last`
Retorna o Ãºltimo resultado em cache.

#### `GET /api/health`
Health check da aplicaÃ§Ã£o.

```json
{
  "status": "OK",
  "timestamp": "2025-11-13T20:28:08.934Z",
  "service": "Selecty Scraper API"
}
```

## ğŸ”„ IntegraÃ§Ã£o com n8n

### ConfiguraÃ§Ã£o do Workflow

1. **Cron Trigger**: Configure para executar nos horÃ¡rios desejados
2. **HTTP Request**: 
   - Method: POST
   - URL: `https://sua-app.easypanel.app/api/scrape`
   - Headers: `Authorization: Bearer YOUR_API_KEY`
   - Timeout: 15 minutos

3. **Processamento**: Use o campo `formattedText` de cada vaga para inserir no Google Sheets

### Exemplo de uso no n8n

```javascript
// No node de processamento, acesse os dados assim:
const vacancies = $json.vacancies;
const formattedData = vacancies.map(v => v.formattedText).join('\n\n');
```

## ğŸ”§ ConfiguraÃ§Ã£o de VariÃ¡veis

| VariÃ¡vel | DescriÃ§Ã£o | Exemplo |
|----------|-----------|---------|
| `SELECTY_EMAIL` | Email de login no Selecty | `usuario@empresa.com` |
| `SELECTY_PASSWORD` | Senha do Selecty | `MinhaSenh@123` |
| `SELECTY_LOGIN_URL` | URL de login | `https://selecty.app/login` |
| `SELECTY_VACANCY_URL` | URL da lista de vagas | `https://selecty.app/vacancy/lists/index` |
| `SCRAPER_TIMEOUT` | Timeout em ms | `30000` |
| `SCRAPER_HEADLESS` | Modo headless | `true` |
| `PORT` | Porta da API | `3000` |
| `API_KEY` | Chave de autenticaÃ§Ã£o | `chave-super-segura-123` |
| `NODE_ENV` | Ambiente | `production` |

## ğŸš¨ Rate Limiting

- **Limite**: 10 requests por hora por IP
- **Window**: 1 hora
- **Headers**: Retorna informaÃ§Ãµes de rate limit nos headers da resposta

## ğŸ” Monitoramento

### Logs

A aplicaÃ§Ã£o gera logs detalhados durante a execuÃ§Ã£o:

```bash
# Ver logs do container
docker logs selecty-scraper -f
```

### Health Check

```bash
curl http://localhost:3000/api/health
```

### Status do Scraper

```bash
curl -H "Authorization: Bearer YOUR_API_KEY" \
     http://localhost:3000/api/scrape/status
```

## ğŸ›¡ï¸ SeguranÃ§a

- **AutenticaÃ§Ã£o obrigatÃ³ria** em todos os endpoints do scraper
- **Rate limiting** para prevenir abuso
- **VariÃ¡veis de ambiente** para credenciais sensÃ­veis
- **CORS configurado** para requisiÃ§Ãµes cross-origin
- **Headers de seguranÃ§a** incluÃ­dos nas respostas

## ğŸ› Troubleshooting

### Erro de autenticaÃ§Ã£o no Selecty
- Verifique se as credenciais estÃ£o corretas no `.env`
- Confirme se a conta nÃ£o estÃ¡ bloqueada

### Timeout durante extraÃ§Ã£o
- Aumente o `SCRAPER_TIMEOUT` para conexÃµes lentas
- Verifique se o site estÃ¡ acessÃ­vel

### Erro 401 na API
- Confirme se o `API_KEY` estÃ¡ correto
- Verifique se o header `Authorization` estÃ¡ sendo enviado

### Container nÃ£o inicia
- Verifique se todas as variÃ¡veis de ambiente estÃ£o definidas
- Confirme se a porta 3000 nÃ£o estÃ¡ em uso

## ğŸ“ Changelog

### v1.0.0
- âœ… Scraper completo funcional
- âœ… API REST com autenticaÃ§Ã£o
- âœ… ExtraÃ§Ã£o de status das vagas
- âœ… Docker e docker-compose
- âœ… Rate limiting
- âœ… Health checks
- âœ… DocumentaÃ§Ã£o completa

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a ISC. Veja o arquivo `LICENSE` para mais detalhes.

## ğŸ“ Suporte

Para suporte e dÃºvidas:
- Abra uma issue no GitHub
- Consulte a documentaÃ§Ã£o da API
- Verifique os logs da aplicaÃ§Ã£o